######################################
Apache Spark
######################################
installed in: [~/Downloads/spark-2.0.1-bin-hadoop2.7/]

http://spark.apache.org/downloads.html
As of the time, I used the default download:

- Download Spark: spark-2.0.1-bin-hadoop2.7.tgz
- then: $ brew install sbt

1. Open README.md. Follow some instructions to check if spark is working ok
2. Follow instructions here:
http://spark.apache.org/docs/latest/quick-start.html
- creating SimpleApp.scala
- creating simple.sbt
-> create a package
-> put SimpleApp.scala in /spark-2.0.1-bin-hadoop2.7/src/main/scala/
- $ sbt package
-> to run
$ ./bin/spark-shell --master local[2]
$ ./bin/spark-shell --master spark://ELIs-Mac-mini.home:7077

$ ./bin/spark-submit   --class "SimpleApp"   --master local[4]   target/scala-2.11/simple-project_2.11-1.0.jar
$ ./bin/spark-submit   --class "SimpleApp"   --master spark://ELIs-Mac-mini.home:7077   target/scala-2.11/simple-project_2.11-1.0.jar

$ ./bin/spark-shell --master spark://ELIs-Mac-mini.home:7077
$ ./bin/spark-shell --master mesos://localhost:5050


------------------------ http://spark.apache.org/docs/2.0.1/spark-standalone.html


http://sonra.io/2015/06/03/multiple-spark-worker-instances-on-a-single-node-why-more-of-less-is-more-than-less/
SPARK_WORKER_INSTANCES=3 SPARK_WORKER_CORES=1 ./sbin/start-slave.sh spark://ELIs-Mac-mini.home:7077
-> open 3 workers, use 1 core each, works OK

./sbin/start-master.sh
./sbin/start-slave.sh spark://ELIs-Mac-mini.home:7077
-> run 1 worker
SPARK_WORKER_INSTANCES=1 SPARK_WORKER_CORES=1 ./sbin/start-slave.sh spark://ELIs-Mac-mini.home:7077
./sbin/stop-all.sh

sample in /conf/spark-env.sh
export SPARK_WORKER_MEMORY=1000m

web ui: http://localhost:8080/

running app on a cluster:
./bin/spark-shell --master spark://ELIs-Mac-mini.home:7077
./bin/spark-submit --master spark://ELIs-Mac-mini.home:7077 --class "SimpleApp" target/scala-2.11/simple-project_2.11-1.0.jar




MASTER=spark://ELIs-Mac-mini.home:7077 ./bin/run-example SparkPi


3 samples running yarn: works OK !!!
./bin/spark-shell --master yarn --deploy-mode client
./bin/spark-submit --master yarn --class "SimpleApp" target/scala-2.11/simple-project_2.11-1.0.jar
./bin/spark-submit --class org.apache.spark.examples.SparkPi \
    --master yarn \
    --deploy-mode cluster \
    --driver-memory 1G \
    --executor-memory 1G \
    --executor-cores 1 \
    examples/jars/spark-examples*.jar \
    10
============================================================================================================================
============================================================================================================================
============================================================================================================================
============================================================================================================================
######################################
Apache Mesos
######################################
/Downloads/mesos-1.0.1/
/Downloads/mesos-1.0.1/build/

http://spark.apache.org/docs/2.0.1/running-on-mesos.html
http://mesos.apache.org/gettingstarted/  --> installing, building, initial tests

web ui: http://localhost:5050/

:to run mesos from spark:
$ cd /spark-2.0.1-bin-hadoop2.7/
$ ./bin/spark-shell --master mesos://localhost:5050

./bin/spark-shell --master mesos://localhost:5050
./bin/spark-shell --master mesos://127.0.0.1:5050

./bin/spark-submit --master mesos://localhost:5050 --class "SimpleApp" target/scala-2.11/simple-project_2.11-1.0.jar

./bin/spark-submit --master mesos://localhost:5050 --class "SimpleApp" target/scala-2.11/simple-project_2.11-1.0.jar  --deploy-mode cluster --supervise

./bin/spark-submit --class org.apache.spark.examples.SparkPi --master mesos://localhost:5050 --deploy-mode cluster --supervise http://localhost/spark_jars/jars/spark-examples_2.11-2.0.1.jar
  
  
to run master and agent: works OK
cd ~/Downloads/mesos-1.0.1/build: 
./bin/mesos-master.sh --ip=127.0.0.1 --work_dir=/var/lib/mesos
./bin/mesos-agent.sh --master=127.0.0.1:5050 --work_dir=/var/lib/mesos

============================================================================================================================
============================================================================================================================
============================================================================================================================
============================================================================================================================
######################################
Apache Hadoop
######################################
[/usr/local/Cellar/hadoop/2.7.3/libexec]

http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html

to fix: ssh error: "connect to host localhost port 22: Connection refused"
http://stackoverflow.com/questions/17335728/connect-to-host-localhost-port-22-connection-refused

brew install hadoop
In Hadoop's config file:
  /usr/local/Cellar/hadoop/2.7.3/libexec/etc/hadoop/hadoop-env.sh,
  /usr/local/Cellar/hadoop/2.7.3/libexec/etc/hadoop/mapred-env.sh and
  /usr/local/Cellar/hadoop/2.7.3/libexec/etc/hadoop/yarn-env.sh
$JAVA_HOME has been set to be the output of:
  /usr/libexec/java_home
==> Summary
í ¼í½º  /usr/local/Cellar/hadoop/2.7.3:

start following steps now:
cd /usr/local/Cellar/hadoop/2.7.3/libexec

--- Standalone Operation: steps works! --- http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Standalone_Operation
$ mkdir input
$ cp etc/hadoop/*.xml input
$ bin/hadoop jar share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.2.jar grep input output 'dfs[a-z.]+'
$ cat output/*

--- Pseudo-Distributed Operation --- http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#Pseudo-Distributed_Operation
etc/hadoop/core-site.xml: {<value>hdfs://localhost:9000</value> change to: <value>hdfs://0.0.0.0:9000</value>}
etc/hadoop/hdfs-site.xml:
**NameNode - http://localhost:50070/ -> works
**DataNode - http://localhost:50075/ -> works

--- YARN on a Single Node --- http://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-common/SingleCluster.html#YARN_on_a_Single_Node
etc/hadoop/mapred-site.xml:
etc/hadoop/yarn-site.xml:
Start ResourceManager daemon and NodeManager daemon:
$ sbin/start-yarn.sh
ResourceManager - http://localhost:8088/
When youâ€™re done, stop the daemons with:
$ sbin/stop-yarn.sh

============================================================================================================================
============================================================================================================================
============================================================================================================================
============================================================================================================================
######################################
Apache Cassandra
######################################
I used the one with: DataStax Distribution of Apache Cassandra
http://www.planetcassandra.org/cassandra/

http://www.planetcassandra.org/apache-cassandra-client-drivers/

============================================================================================================================
============================================================================================================================
============================================================================================================================
============================================================================================================================


